AWSTemplateFormatVersion: '2010-09-09'
Description: 'AWS CloudFormation Template for Bedrock API Usage Reporting Infrastructure'

Resources:
  # S3 bucket for storing usage data
  UsageDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'bedrock-usage-data-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled

  # IAM role for CloudWatch Export
  CloudWatchExportRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
      Policies:
        - PolicyName: CloudWatchExportPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'cloudwatch:GetMetricData'
                  - 'cloudwatch:GetMetricStatistics'
                  - 'cloudwatch:ListMetrics'
                Resource: '*'
              - Effect: Allow
                Action:
                  - 's3:PutObject'
                Resource: !Sub '${UsageDataBucket.Arn}/*'
              - Effect: Allow
                Action:
                  - 'apigateway:GET'
                Resource: '*'

  # Lambda function to export API usage to S3
  ExportUsageLambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt CloudWatchExportRole.Arn
      Runtime: python3.8
      Timeout: 300
      Code:
        ZipFile: |
          import boto3
          import json
          import os
          import datetime
          import csv
          import io
          
          def handler(event, context):
              # Get yesterday's date
              yesterday = datetime.datetime.now() - datetime.timedelta(days=1)
              date_str = yesterday.strftime('%Y-%m-%d')
              
              # Initialize clients
              apigateway = boto3.client('apigateway')
              cloudwatch = boto3.client('cloudwatch')
              s3 = boto3.client('s3')
              
              # Get all usage plans
              usage_plans = apigateway.get_usage_plans()['items']
              
              # For each usage plan, get all API keys
              for plan in usage_plans:
                  plan_id = plan['id']
                  plan_name = plan['name']
                  
                  # Get all API keys for this plan
                  try:
                      keys = apigateway.get_usage_plan_keys(usagePlanId=plan_id)['items']
                  except Exception as e:
                      print(f"Error getting keys for plan {plan_id}: {str(e)}")
                      continue
                  
                  # Create a CSV for this plan
                  output = io.StringIO()
                  writer = csv.writer(output)
                  writer.writerow(['Date', 'PlanId', 'PlanName', 'KeyId', 'KeyName', 'Requests'])
                  
                  # For each key, get usage data
                  for key in keys:
                      key_id = key['id']
                      key_name = key['name']
                      
                      try:
                          # Get usage data for yesterday
                          usage = apigateway.get_usage(
                              usagePlanId=plan_id,
                              keyId=key_id,
                              startDate=date_str,
                              endDate=date_str
                          )
                          
                          # Extract usage data
                          if 'items' in usage and key_id in usage['items']:
                              usage_data = usage['items'][key_id]
                              for api_id in usage_data:
                                  for date, count in usage_data[api_id].items():
                                      writer.writerow([date, plan_id, plan_name, key_id, key_name, count])
                          
                      except Exception as e:
                          print(f"Error getting usage for key {key_id}: {str(e)}")
                          continue
                  
                  # Upload CSV to S3
                  bucket_name = os.environ['BUCKET_NAME']
                  s3.put_object(
                      Bucket=bucket_name,
                      Key=f'usage/{plan_name}/{date_str}.csv',
                      Body=output.getvalue(),
                      ContentType='text/csv'
                  )
                  
                  print(f"Exported usage data for plan {plan_name} to S3")
              
              return {
                  'statusCode': 200,
                  'body': json.dumps('Export completed successfully')
              }
      Environment:
        Variables:
          BUCKET_NAME: !Ref UsageDataBucket

  # CloudWatch Event Rule to trigger the export daily
  DailyExportRule:
    Type: AWS::Events::Rule
    Properties:
      Description: 'Trigger daily API usage export'
      ScheduleExpression: 'cron(0 1 * * ? *)'  # Run at 1:00 AM UTC daily
      State: ENABLED
      Targets:
        - Arn: !GetAtt ExportUsageLambda.Arn
          Id: 'ExportUsageLambdaTarget'

  # Permission for CloudWatch Events to invoke Lambda
  PermissionForEventsToInvokeLambda:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ExportUsageLambda
      Action: 'lambda:InvokeFunction'
      Principal: 'events.amazonaws.com'
      SourceArn: !GetAtt DailyExportRule.Arn

  # Glue Database for usage data
  UsageDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: bedrock_usage_db
        Description: 'Database for Bedrock API usage data'

  # Glue Crawler to catalog usage data
  UsageCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: bedrock-usage-crawler
      Role: !GetAtt GlueServiceRole.Arn
      DatabaseName: !Ref UsageDatabase
      Targets:
        S3Targets:
          - Path: !Sub 's3://${UsageDataBucket}/usage/'
      Schedule:
        ScheduleExpression: 'cron(0 2 * * ? *)'  # Run at 2:00 AM UTC daily
      SchemaChangePolicy:
        UpdateBehavior: 'UPDATE_IN_DATABASE'
        DeleteBehavior: 'DELETE_FROM_DATABASE'

  # IAM role for Glue service
  GlueServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole'
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:ListBucket'
                Resource:
                  - !GetAtt UsageDataBucket.Arn
                  - !Sub '${UsageDataBucket.Arn}/*'

Outputs:
  UsageDataBucketName:
    Description: 'Name of the S3 bucket storing usage data'
    Value: !Ref UsageDataBucket
  
  GlueDatabase:
    Description: 'Glue Database for querying usage data'
    Value: !Ref UsageDatabase
  
  SetupComplete:
    Description: 'Next steps'
    Value: 'Infrastructure setup complete. Connect Amazon QuickSight to the Glue Database to visualize usage.'
