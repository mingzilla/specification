AWSTemplateFormatVersion: '2010-09-09'
Description: 'AWS CloudFormation Template for Bedrock API Usage Reporting Infrastructure (Bearer Token Authentication)'

Resources:
  # S3 bucket for storing usage data
  UsageDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'bedrock-usage-data-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled

  # IAM role for CloudWatch Log processing
  CloudWatchLogsProcessorRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
      Policies:
        - PolicyName: CloudWatchLogsProcessorPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'logs:StartQuery'
                  - 'logs:GetQueryResults'
                  - 'logs:DescribeLogGroups'
                  - 'logs:DescribeLogStreams'
                Resource: '*'
              - Effect: Allow
                Action:
                  - 's3:PutObject'
                Resource: !Sub '${UsageDataBucket.Arn}/*'
              - Effect: Allow
                Action:
                  - 'dynamodb:Scan'
                  - 'dynamodb:GetItem'
                Resource: 
                  - !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/bedrock_customers'
                  - !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/bedrock_tokens'

  # Lambda function to export usage data from CloudWatch Logs to S3
  ExportUsageLambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt CloudWatchLogsProcessorRole.Arn
      Runtime: python3.8
      Timeout: 900  # 15 minutes for long-running log queries
      MemorySize: 512
      Code:
        ZipFile: |
          import boto3
          import json
          import os
          import time
          import datetime
          import csv
          import io
          
          def handler(event, context):
              print("Starting usage data export")
              
              # Get yesterday's date or use date from event if provided
              if event and 'date' in event:
                  date_str = event['date']
                  date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')
              else:
                  date_obj = datetime.datetime.now() - datetime.timedelta(days=1)
                  date_str = date_obj.strftime('%Y-%m-%d')
              
              # Initialize clients
              logs = boto3.client('logs')
              dynamodb = boto3.resource('dynamodb')
              s3 = boto3.client('s3')
              
              # Get all customers
              customer_table = dynamodb.Table('bedrock_customers')
              customer_response = customer_table.scan()
              customers = customer_response.get('Items', [])
              
              # Get log group for Lambda function
              log_group_name = '/aws/lambda/bedrock-proxy'  # Update with your Lambda log group name
              
              # Calculate time range for yesterday
              start_time = int(datetime.datetime(date_obj.year, date_obj.month, date_obj.day, 0, 0, 0).timestamp() * 1000)
              end_time = int(datetime.datetime(date_obj.year, date_obj.month, date_obj.day, 23, 59, 59).timestamp() * 1000)
              
              # Process logs for each customer
              all_customer_data = []
              for customer in customers:
                  customer_id = customer['customer_id']
                  customer_name = customer['name']
                  print(f"Processing logs for customer: {customer_name} ({customer_id})")
                  
                  # Updated query to match the Lambda's log format
                  query = f"""
                      fields @timestamp, @message
                      | parse @message '{{"event_type":"*","customer_id":"*","request_id":"*","model_id":"*","input_tokens":*,"output_tokens":*,"token_count":*,"duration_ms":*,"timestamp":*}}' as event_type, customer_id, request_id, model_id, input_tokens, output_tokens, token_count, duration_ms, timestamp
                      | filter customer_id = '{customer_id}' and event_type = 'bedrock_invoke'
                      | stats 
                          count(*) as request_count, 
                          sum(token_count) as total_tokens
                          by bin(1h) as hour, model_id
                  """
                  
                  try:
                      # Start query
                      start_query_response = logs.start_query(
                          logGroupName=log_group_name,
                          startTime=start_time,
                          endTime=end_time,
                          queryString=query,
                          limit=10000
                      )
                      
                      query_id = start_query_response['queryId']
                      
                      # Wait for query to complete
                      response = None
                      status = 'Running'
                      
                      # Poll for results
                      max_attempts = 60  # 10 minutes max
                      attempt = 0
                      
                      while status == 'Running' and attempt < max_attempts:
                          time.sleep(10)  # Wait 10 seconds between calls
                          response = logs.get_query_results(queryId=query_id)
                          status = response['status']
                          attempt += 1
                      
                      # Process results
                      if status == 'Complete':
                          results = response['results']
                          
                          # Aggregate results by hour and model
                          customer_data = []
                          for result in results:
                              result_dict = {field['field']: field['value'] for field in result}
                              
                              customer_data.append({
                                  'date': date_str,
                                  'customer_id': customer_id,
                                  'customer_name': customer_name,
                                  'hour': result_dict.get('hour', 'unknown'),
                                  'model_id': result_dict.get('model_id', 'unknown'),
                                  'request_count': int(result_dict.get('request_count', '0')),
                                  'total_tokens': int(result_dict.get('total_tokens', '0'))
                              })
                          
                          all_customer_data.extend(customer_data)
                          print(f"Found {len(customer_data)} hourly records for {customer_name}")
                      
                      else:
                          print(f"Query did not complete in time. Status: {status}")
                  
                  except Exception as e:
                      print(f"Error processing logs for customer {customer_id}: {str(e)}")
              
              # Write all data to CSV
              if all_customer_data:
                  print(f"Writing {len(all_customer_data)} records to S3")
                  
                  output = io.StringIO()
                  writer = csv.DictWriter(output, fieldnames=[
                      'date', 'customer_id', 'customer_name', 'hour', 'model_id', 
                      'request_count', 'total_tokens'
                  ])
                  writer.writeheader()
                  
                  for record in all_customer_data:
                      writer.writerow(record)
                  
                  # Upload CSV to S3
                  bucket_name = os.environ['BUCKET_NAME']
                  s3.put_object(
                      Bucket=bucket_name,
                      Key=f'usage/{date_str}/daily_usage.csv',
                      Body=output.getvalue(),
                      ContentType='text/csv'
                  )
                  
                  print(f"Successfully exported usage data for {date_str}")
                  
                  # Create a monthly summary if this is end of month
                  is_end_of_month = (date_obj + datetime.timedelta(days=1)).day == 1
                  if is_end_of_month:
                      # Create monthly summary (implementation left as an exercise)
                      # This would involve querying the entire month's data
                      print("Creating monthly summary...")
              
              else:
                  print("No usage data found for any customers")
              
              return {
                  'statusCode': 200,
                  'body': json.dumps(f'Export completed for {date_str}')
              }
      Environment:
        Variables:
          BUCKET_NAME: !Ref UsageDataBucket

  # CloudWatch Event Rule to trigger the export daily
  DailyExportRule:
    Type: AWS::Events::Rule
    Properties:
      Description: 'Trigger daily API usage export'
      ScheduleExpression: 'cron(0 1 * * ? *)'  # Run at 1:00 AM UTC daily
      State: ENABLED
      Targets:
        - Arn: !GetAtt ExportUsageLambda.Arn
          Id: 'ExportUsageLambdaTarget'

  # Permission for CloudWatch Events to invoke Lambda
  PermissionForEventsToInvokeLambda:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ExportUsageLambda
      Action: 'lambda:InvokeFunction'
      Principal: 'events.amazonaws.com'
      SourceArn: !GetAtt DailyExportRule.Arn

  # Glue Database for usage data
  UsageDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: bedrock_usage_db
        Description: 'Database for Bedrock API usage data with Bearer token authentication'

  # Glue Crawler to catalog usage data
  UsageCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: bedrock-usage-crawler
      Role: !GetAtt GlueServiceRole.Arn
      DatabaseName: !Ref UsageDatabase
      Targets:
        S3Targets:
          - Path: !Sub 's3://${UsageDataBucket}/usage/'
      Schedule:
        ScheduleExpression: 'cron(0 2 * * ? *)'  # Run at 2:00 AM UTC daily
      SchemaChangePolicy:
        UpdateBehavior: 'UPDATE_IN_DATABASE'
        DeleteBehavior: 'DELETE_FROM_DATABASE'

  # IAM role for Glue service
  GlueServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole'
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:ListBucket'
                Resource:
                  - !GetAtt UsageDataBucket.Arn
                  - !Sub '${UsageDataBucket.Arn}/*'

Outputs:
  UsageDataBucketName:
    Description: 'Name of the S3 bucket storing usage data'
    Value: !Ref UsageDataBucket
  
  GlueDatabase:
    Description: 'Glue Database for querying usage data'
    Value: !Ref UsageDatabase
  
  SetupComplete:
    Description: 'Next steps'
    Value: 'Infrastructure setup complete. Connect Amazon QuickSight to the Glue Database to visualize usage.'